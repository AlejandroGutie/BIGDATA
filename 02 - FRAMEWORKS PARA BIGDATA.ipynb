{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02 - FRAMEWORKS PARA BIGDATA.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNcs/jl5n06phSdSdL7Fwjb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"FUKq9k8_IRvI"},"source":["!wget --no-cache -O init.py -q https://raw.githubusercontent.com/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-03-BIGDATA/master/init.py\n","import init; init.init(force_download=False); \n","from IPython.display import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQBM7AIvIYyi"},"source":["Image(\"local/imgs/udea-datascience.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PTtKQgTIg69"},"source":["#**FRAMEWORKS PARA BIG DATA**\n","\n","Para implementar las tareas del Big Data suelen utilizarse Frameworks que proponen esquemas y proceso estándar facilitando el trabajo del desarrollador. \n","\n","Revisemos las características de dos de estos Frameworks que ha tenido una gran acogida por los desarrolladores de Big Data"]},{"cell_type":"markdown","metadata":{"id":"lpw3xJJKzb58"},"source":["#**Hadoop**\n","\n","Para muchos es considerado un sinónimo de Big Data\n","\n","Se trata de un proyecto de código abierto desarrollado en lenguaje Java que tuvo sus inicios en 2006 y proporciona una plataforma para almacenamiento y procesamiento de grandes volúmenes de datos de forma escalable y distribuida.\n","\n","Hadoop opera bajo el concepto de clúster, en donde dispone de un coordinador de clúster (namenode) y unos trabajadores (datanode) que interactúan entre sí para resolver problemas complejos de almacenamiento y procesamiento de datos.\n","\n","Si se requiere mayor capacidad de almacenamiento o de procesamiento basta con almacenar más máquinas al clúster en un esquema de escalabilidad horizontal\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ibM5F8wD-VwB"},"source":["Image(\"local/imgs/cluster.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlhaKNVf3rpJ"},"source":["Hadoop está compuesto por un conjunto de herramientas que permiten el almacenamiento y procesamiento de los datos.\n","\n","A continuación, se presenta el ecosistema de Hadoop"]},{"cell_type":"code","metadata":{"id":"bzHmPZMq3gYk"},"source":["Image(\"local/imgs/hadoop.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"713mYQsHzk5Z"},"source":["##**HDFS**\n","\n","Hadoop Distributed File System (HDFS), permite almacenar grandes volúmenes de datos de forma distribuida y con redundancia\n","\n","Se pueden almacenar muchos más datos de forma distribuida que en una sola máquina, con la ventaja de que si una de las máquinas falla el sistema sigue funcionando correctamente.\n"]},{"cell_type":"markdown","metadata":{"id":"5fnQpFoC8fNy"},"source":["Para lograr esto HDFS utiliza la siguiente arquitectura"]},{"cell_type":"code","metadata":{"id":"lHkTB0c98mzF"},"source":["Image(\"local/imgs/hdfs.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iK3qxqU8-sWX"},"source":["En HDFS los datos a almacenar son divididos en bloques y cada uno de los bloques se almacenan con réplica es decir que queda almacenado en varios datanode. Esto es lo que permite que si uno de los datanode deja de estar disponible, la información que almacena se pueda recuperar de otro datanode.\n","\n","Veamos un ejemplo del almacenamiento de datos en HDFS"]},{"cell_type":"code","metadata":{"id":"u-J6qNS-_7r8"},"source":["Image(\"local/imgs/hdfs_ejemplo.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vUo1h1EqpdUI"},"source":["**Ejercicio**\n","\n","Lea el siguiente documento\n"," http://hadoop.apache.org/docs/stable1/hdfs_design.pdf\n","\n"," Y responda a estas preguntas:\n","\n","1.\tNormalmente, de qué tamaño son los archivos que se almacenan en el sistema HDFS?\n","\n","2.\tQué es mas recomendado, mover la aplicación hacia donde están los datos o mover los datos hacia donde está la aplicación?\n","\n","3.\tCuál es el sistema operativo utilizado típicamente para soluciones HDFS?\n","\n","4.\tSi un usuario solicita un archivo que tiene réplicas en varios nodos, cual es el nodo preferencial para responder la solicitud?\n","\n","5.\tEl DataNode almacena todos los archivos en el mismo directorio? Porque?\n","\n","6.\tCuáles son las fallas mas comunes en un sistema HDFS?\n","\n","7.\tCómo se comprueba la integridad de un archivo HDFS?\n","\n","8.\tAl almacenar un archivo quién se encarga de transferir los bloques de datos a los DataNode que van a contener réplicas?\n","\n","9.\tEs posible recuperar un archivo eliminado en el sistema HDFS?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6bh8qM5XzpQQ"},"source":["##**Map Reduce**"]},{"cell_type":"markdown","metadata":{"id":"hcA-lbjOAs9V"},"source":["Metodología de programación dada a conocer por Google orientada a procesamiento en paralelo. \n","\n","Consiste en dividir la aplicación en pequeños fragmentos de trabajo donde cada uno de ellos puede ser ejecutado en un nodo\n","\n","Contempla las siguientes etapas\n","* Split: Divide los datos en múltiples fracciones de datos\n","* Map: Procesamiento de cada nodo\n","* Shuffle & Sort: Ordenar los resultados \n","* Redduce: Compactar y resumir los resultados\n","\n","En esta metodología los datos son entregados en tuplas con formato (clave, valor) \n"]},{"cell_type":"code","metadata":{"id":"_-6Mu8FbBk6m"},"source":["Image(\"local/imgs/map_reduce_key_value.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOQTulMsB3UA"},"source":["Veamos un ejemplo sencillo que consiste en contar cuantas veces se repite cada palabra dentro de un texto dado"]},{"cell_type":"code","metadata":{"id":"V_sLxG3wClY5"},"source":["Image(\"local/imgs/wordcount.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xA1uBaktzxKn"},"source":["##**Otras Herramientas**\n","\n","Revisemos otras de las herramientas del ecosistema Hadoop  "]},{"cell_type":"markdown","metadata":{"id":"FkvyXGjJERqn"},"source":["###**HBase**\n","\n","Es una base de datos NoSQL de tipo Key-Value orientada a columnas, desarrollada en Java que se ejecuta directamente sobre el HDFS\n","\n","El acceso a los datos se puede hacer desde herramientas como Pig o Hive. Las operaciones contra la base de datos se pueden convertir en procesos map reduce"]},{"cell_type":"markdown","metadata":{"id":"8Vla5s-FFJZA"},"source":["###**Pig**\n","\n","Es una plataforma para analizar y consultar grandes volúmenes de datos almacenados en HDFS que permite crear programas Map Reduce, utilizando el lenguaje PigLatin (similar a SQL)\n","\n","Al escribir un Script en PigLatin este es automáticamente paralelizado y distribuido en un clúster\n"]},{"cell_type":"markdown","metadata":{"id":"xDaqdnC_FmE_"},"source":["###**Hive**\n","Es la infraestructura de data warehouse de Hadoop (No es BD)\n","\n","Realiza funciones de agrupación, consulta y análisis de datos, almacenados bajo HDFS implementando las queries mediante el lenguaje HiveQL (HQL)\n","\n","Básicamente traduce consultas tipos SQL a trabajos Map Reduce\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xM3vxwqpGVON"},"source":["###**Sqoop**\n","\n","Sqoop (SQL + Hadoop): Herramienta para transferir datos entre bases de datos relacionales y Hadoop\n","\n","Es decir que permite importar y exportar datos\n","* Transformar datos de una BD relacional a HDFS (Hive o Hbase)\n","* Transformar datos de HDFS a BD relacionales (MySQL, Oracle, etc)\n"]},{"cell_type":"markdown","metadata":{"id":"0t5RqsR4GwKO"},"source":["###**Hue**\n","\n","Editor Open Source que permite consultar y visualizar datos en el ecosistema de Hadoop\n","\n","Permite interactuar con aplicaciones HDFS y Map Reduce  \n"]},{"cell_type":"markdown","metadata":{"id":"eW1WEaZNG8QW"},"source":["###**Mahout**\n","\n","Mahout es una librería de Machine Learning diseñada para operar con HDFS y MapReduce.\n","\n","Incluye varios algoritmos para realizar tareas de clasificación, clustering y filtrado colaborativo"]},{"cell_type":"markdown","metadata":{"id":"jpYiDtT0zf1z"},"source":["#**Spark**\n","\n","Spark es un Framework para Big Data de código abierto que permite trabajar en ambientes distribuidos de forma paralelizada con un gran rendimiento. \n","\n","Puede llegar a ser 100 veces más rápido que Hadoop dado que trabaja en memoria y no en disco\n"]},{"cell_type":"code","metadata":{"id":"oTBgnc97jiZh"},"source":["Image(\"local/imgs/hadoop_spark.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcOx_VU0z5in"},"source":["##**Arquitectura**"]},{"cell_type":"markdown","metadata":{"id":"0JZSlRPekN5x"},"source":["Está compuesto por cuatro grandes librerías:\n","\n","* Spark SQL: Módulo para trabajar con datos estructurados y  semi estructurados\n","* Spark Streaming: Herramientas que permiten la ingesta de datos en tiempo real\n","* Spark MLlib: Librería de Machine Learning\n","* Graph X: Api para trabajar con grafos"]},{"cell_type":"code","metadata":{"id":"YUtNLUJ1mfup"},"source":["Image(\"local/imgs/spark.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oi4Zcv35z-WH"},"source":["##**Lenguajes**"]},{"cell_type":"markdown","metadata":{"id":"6o51AjkemhrH"},"source":["Una de las ventajas que trae Spark, es que se puede utilizar desde varios lenguajes de programación:\n","\n","* Scala\n","* Python\n","* R\n","* Java\n","\n","Esta es una tabla comparativa sobre la operación de Spark en esos lenguajes"]},{"cell_type":"code","metadata":{"id":"UlU4jwZVnimA"},"source":["Image(\"local/imgs/spark_lenguajes.png\")"],"execution_count":null,"outputs":[]}]}